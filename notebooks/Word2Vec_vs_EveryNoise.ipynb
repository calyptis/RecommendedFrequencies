{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bda3c45-d863-431b-adb8-22d305122292",
   "metadata": {},
   "source": [
    "# About this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee2c4d-c3d3-4dec-aa96-3b52f2d40e4e",
   "metadata": {},
   "source": [
    "This notebooks investigates how well a pre-trained `word2vec` model from FastText can capture genre similarities when treating individual genres as words or sentences. The model is tested as is, with no fine-tuning on music related documents.\n",
    "\n",
    "Each genre, for example `Rock`, is treated as a word and its embedding obtained by the pre-trained model.\n",
    "\n",
    "Genre similarities are then just the cosine distance between their respective word embeddings.\n",
    "\n",
    "To investigate how well these embedding similarities reflect musical properties, genre similarities as reported by [`everynoise`](https://everynoise.com) are used for comparison.\n",
    "\n",
    "This website clusters genres based on musical attributes and thus provides a good baseline.\n",
    "\n",
    "The methodology of this analysis is as follows:\n",
    "\n",
    "- Select N genres, where N is either\n",
    "    - most frequently occurring genres based on songs featured on Spotify's curated playlists\n",
    "    - most frequently occurring genres in a user's library\n",
    "    - manually curated list of genres\n",
    "- For each pair of these N genres, calculate the cosine distance of `word2vec` embeddings and euclidean distance of `everynoise` embeddings. The latter is effectively based on the `(x, y)` position of the genres in the 2D embedding space.\n",
    "- For each genre, rank all other genres in terms of decreasing similarity, with the most similar genre assigned the 1st rank and the least similar genre the Nth rank. This creates a ranking for each of the two methods, `word2vec` and `everynoise`.\n",
    "- Compare the absolute difference in ranking between the two methods. A higher difference suggests that the two methods disagree on a given genre similarity. For example, if `Rock` is the 2nd most similar genre to `Hard Rock` according to `everynoise` but the 50th according to `word2vec`, the two methods for measuring genre similarity lead to different results.\n",
    "- Assuming that `everynoise` is a faithful representation of genre similarity, we can judge how well `word2vec` performs for the same task by comparing the ranking difference.\n",
    "\n",
    "As the heatmap in this notebook shows, the rankings of the two methods are frequently quite different, suggesting that `word2vec` is a poor representation of genre similarities.\n",
    "\n",
    "This is further backed by a two-dimensional representation of both embedding spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ade142-b48f-4020-a74b-6cb1f98c4d31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c34d80-0782-4bdb-abe1-d2202b0daba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ab8e7-c486-4a1f-9cd8-f1e31bbbd399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import fasttext\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pylab as plt\n",
    "import plotly.express as px\n",
    "import scipy.stats as stats\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from functools import reduce\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fed777-46e2-441f-8dd5-9b00ade37e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f22ca6-67f9-496a-92b8-658492c655d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommended_frequencies.config import DATA_DIR, MODEL_DIR\n",
    "from recommended_frequencies.spotify.config import CO_OCCURRENCE_TABLE\n",
    "from recommended_frequencies.spotify.utils import get_frequent_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302bccac-c97d-4438-b4a8-4674deae5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURATED_DIR = os.path.join(DATA_DIR, \"Curated_Playlists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02ad6d4-70ba-45bc-8a05-a3ad4f5e82e3",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2326017-37c0-4a6d-aa7c-aa67076c22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained FastText model\n",
    "ft = fasttext.load_model(os.path.join(MODEL_DIR, 'cc.en.300.bin'))\n",
    "\n",
    "# Number of genres to include\n",
    "N = 50\n",
    "\n",
    "# List of genres by descending order of occurrence\n",
    "# A) Occurrence based on a user's library\n",
    "# freq_genres = get_frequent_genres(thr=N)\n",
    "# B) Occurrence based on Spotify's curated playlists\n",
    "#    Focus on the top N genres\n",
    "# freq_genres = pd.read_csv(os.path.join(CURATED_DIR, \"curated_playlists_genres.csv\")).genre.head(N)\n",
    "# C) Hand-picked genres\n",
    "freq_genres = [i.strip() for i in open(os.path.join(DATA_DIR, \"manually_picked_genres.txt\"), \"r\").readlines()]\n",
    "N = len(freq_genres)  # Overwrite N\n",
    "\n",
    "# Everynoise genre embedding\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, \"genre_space_everynoise.csv\"))\n",
    "\n",
    "# Only analyse frequently occurring genres\n",
    "df = df.loc[lambda x: x.genre.isin(freq_genres)].copy()\n",
    "\n",
    "# Sort these genres by their everynoise embedding\n",
    "# Because those close according to their (x,y) position are musically similar\n",
    "# This helps keeping similar genres close together in the heatmap at the end of the notebook\n",
    "# Use everynoise sorting\n",
    "# df.sort_values(by=[\"x\", \"y\"], inplace=True)  \n",
    "# Use custom sorting\n",
    "order = {v:i for i, v in enumerate(freq_genres)}\n",
    "df[\"tmp\"] = df.genre.map(order)\n",
    "df.sort_values(by=\"tmp\", inplace=True)\n",
    "df.drop(columns=[\"tmp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f693a56-c910-4ad0-aa88-54f15c091cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.genre.nunique(), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d3f6d-ac10-4b47-9ac3-bb65e2f80fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize them to [0, 1]\n",
    "df[\"x\"] /= df[\"x\"].max()\n",
    "df[\"y\"] /= df[\"y\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa9b0b-684d-44de-9921-24da11ed81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare everynoise embedding, i.e. (x,y) position as array\n",
    "df[\"everynoise_embedding\"] = df.apply(lambda row: np.array([row.x, row.y]), axis=1)\n",
    "# Get word2vec embedding by treating genre as a sentence \n",
    "# because some genres are a composite of several words, like \"background music\"\n",
    "df[\"word2vec_embedding\"] = df.genre.apply(ft.get_sentence_vector)\n",
    "# df.drop(columns=[\"x\", \"y\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ee62d-e7a4-404d-b0ed-92d74f09ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform embeddings into a numpy matrix\n",
    "matrix_everynoise = np.stack(df.everynoise_embedding.values)\n",
    "matrix_word2vec = np.stack(df.word2vec_embedding.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb310c8-d23f-41ea-98d7-60af87dfe483",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"genre\"] = df[\"genre\"].str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670f261d-fee9-4dea-a921-38e8423bfc5a",
   "metadata": {},
   "source": [
    "# Calculate genre similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c35d39-84d1-4cbb-9914-4d2623b5824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everynoise embedding similarity based on Euclidean distance\n",
    "df_everynoise = pd.DataFrame(\n",
    "    euclidean_distances(matrix_everynoise),\n",
    "    index=df.genre.values,\n",
    "    columns=df.genre.values\n",
    ")\n",
    "# Distance to similarity\n",
    "df_everynoise.loc[:] = 1 / (1 + df_everynoise.values)\n",
    "\n",
    "# Word2vec embedding similarity based on Cosine similarity\n",
    "df_word2vec = pd.DataFrame(\n",
    "    cosine_similarity(matrix_word2vec),\n",
    "    index=df.genre.values,\n",
    "    columns=df.genre.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335c863-3371-454e-85dd-6ed9bdad24d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_everynoise.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e119a-c301-4c37-adf8-144bf2cd7c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word2vec.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d9a809-85d8-457d-b3bd-29fda7d29c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance matrices are symmetric\n",
    "assert np.allclose(df_everynoise, df_everynoise.T, rtol=1e-05, atol=1e-08)\n",
    "assert np.allclose(df_word2vec, df_word2vec.T, rtol=1e-05, atol=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2f603-b5f9-4887-806a-0dcb7727cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform distance matrix into a ranking matrix:\n",
    "# Given a genre, the closest genre to it will be given rank equal to 1\n",
    "# and the genre most different a rank equal to N\n",
    "\n",
    "df_everynoise_ranked = pd.DataFrame(\n",
    "    np.apply_along_axis(stats.rankdata, arr=-1 * df_everynoise.values, axis=1),\n",
    "    index=df_everynoise.index,\n",
    "    columns=df_everynoise.columns\n",
    ").astype(int)\n",
    "\n",
    "df_word2vec_ranked = pd.DataFrame(\n",
    "    np.apply_along_axis(stats.rankdata, arr=-1 * df_word2vec.values, axis=1),\n",
    "    index=df_word2vec.index,\n",
    "    columns=df_word2vec.columns\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7a4aa-02d7-41b4-ab7d-4b7541fb300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_everynoise_ranked.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa517836-aabd-4c78-a18a-3acb2bdc79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word2vec_ranked.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c69f58-d73f-405d-b84a-226a2987f8dd",
   "metadata": {},
   "source": [
    "Note that entries in the diagonals are equal to 1 because distance between the two genres is 0.\n",
    "\n",
    "The lower the ranking, the more similar the two genres are.\n",
    "\n",
    "The highest ranking is equal to 100 because we have only inlcuded 100 genres in this analysis.\n",
    "\n",
    "Looking at the second table (`word2vec`) we see that white noise is not similar to the genre metalcore at all (rank 88).\n",
    "\n",
    "There are other 87 genres that are more similar to white noise than metalcore is.\n",
    "\n",
    "In fact, background music is the most similar genre (rank 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375748d8-d7f7-4fc1-b7aa-99314962ab8e",
   "metadata": {},
   "source": [
    "# Visualise word2vec vs everynoise genre similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b395af-5890-422a-94c4-9a595d86d84d",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b264cb87-9789-4c10-ad9b-6eb870d89a8e",
   "metadata": {},
   "source": [
    "First, let's project word2vec genre embeddings onto a 2D space and see if genres cluster similarly across the two approaches. Note that Every Noise embeddings are already in a 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c87c5-12ec-477f-b22b-2439b838f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.everynoise_embedding.apply(lambda x: x[0]).values\n",
    "y = df.everynoise_embedding.apply(lambda x: x[1]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0478ae-1e6f-4a09-8520-41de2bb6856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually move the location of some genres by a small amount to ensure that the\n",
    "# labels in the plot do not end up overlapping\n",
    "y_alt = y.copy()\n",
    "for g, idx in zip(df.genre, range(y.shape[0])):\n",
    "    if g == \"Edm\":\n",
    "        y_alt[idx] -= 0.05\n",
    "    elif g == \"Tropical House\":\n",
    "        y_alt[idx] -= 0.1\n",
    "    elif g == \"House\":\n",
    "        y_alt[idx] -= 0.05\n",
    "    elif g == \"Grime\":\n",
    "        y_alt[idx] += 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b08df5-93d7-4601-a343-84bb2fc5a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    x=x, \n",
    "    y=y_alt, \n",
    "    text=df.genre.str.title(), size=np.zeros(len(df)),\n",
    "    width=1200,\n",
    "    height=1000,\n",
    "    title=\"Every Noise At Once Embedding Space\"\n",
    ")\n",
    "fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', \n",
    "                  xaxis=dict(showticklabels=False), \n",
    "                  yaxis=dict(showticklabels=False), \n",
    "                  xaxis_title=\"X\", \n",
    "                  yaxis_title=\"Y\",\n",
    "                  font=dict(\n",
    "                    family=\"avenir, sans serif\",\n",
    "                    size=24,\n",
    "                    color=\"#334c57\"\n",
    "                  )\n",
    ")\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black', range=[-0.1, 1.1])\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2042d-2dfe-4b8e-93bf-ca400371312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Github: visualise a static version of the above Plotly figure\n",
    "from IPython.display import Image\n",
    "img_bytes = fig.to_image(format=\"png\")\n",
    "Image(img_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834eeea-5d1a-4239-8c83-f34170a2b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, learning_rate=\"auto\", init=\"random\", perplexity=5, random_state=2022)\n",
    "word2vec_tsne_2D = tsne.fit_transform(np.stack(df.word2vec_embedding.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907222d6-1d4b-44d7-b56e-c176b8c9f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = word2vec_tsne_2D[:, 0]\n",
    "y = word2vec_tsne_2D[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a743f11-060c-4c18-9891-75cc0cb91094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually move the location of some genres by a small amount to ensure that the\n",
    "# labels in the plot do not end up overlapping\n",
    "y_alt = y.copy()\n",
    "for g, idx in zip(df.genre, range(y.shape[0])):\n",
    "    if g == \"Hip Hop\":\n",
    "        y_alt[idx] += 0.8\n",
    "    elif g == \"Deep House\":\n",
    "        y_alt[idx] -= 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc710643-e33a-4070-9393-baec305b6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    x=x, \n",
    "    y=y_alt, \n",
    "    text=df.genre.str.title(), \n",
    "    size=np.zeros(len(df)),\n",
    "    width=1200,\n",
    "    height=1000,\n",
    "    title=\"Word2Vec Embedding Space Projected in 2D Using t-SNE\"\n",
    ")\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='rgba(0,0,0,0)', \n",
    "    xaxis=dict(showticklabels=False), \n",
    "    yaxis=dict(showticklabels=False), \n",
    "    xaxis_title=\"X\", \n",
    "    yaxis_title=\"Y\",\n",
    "    font=dict(\n",
    "        family=\"avenir, sans serif\",\n",
    "        size=24,\n",
    "        color=\"#334c57\"\n",
    "    )\n",
    ")\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92afd4-d056-4b9b-afef-6ffe029c6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Github: visualise a static version of the above Plotly figure\n",
    "from IPython.display import Image\n",
    "img_bytes = fig.to_image(format=\"png\")\n",
    "Image(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87713a0-8c2e-4d06-b9b3-f8fc488acfbf",
   "metadata": {},
   "source": [
    "## Comparison of a single genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2381943-a836-4870-9239-8c4e6bbd0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_genre = \"Rap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4411e-1f58-4661-b2a2-d0659569ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarities\n",
    "subset_everynoise = df_everynoise.loc[chosen_genre, df_everynoise.columns.difference([chosen_genre])]\n",
    "subset_word2vec = df_word2vec.loc[chosen_genre, df_word2vec.columns.difference([chosen_genre])]\n",
    "\n",
    "# Rankings\n",
    "subset_everynoise_ranked = df_everynoise_ranked.loc[chosen_genre, df_everynoise_ranked.columns.difference([chosen_genre])] - 1\n",
    "subset_word2vec_ranked = df_word2vec_ranked.loc[chosen_genre, df_word2vec_ranked.columns.difference([chosen_genre])] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3371133-c732-44d5-a97a-1d36e8f71676",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = [\n",
    "    subset_everynoise.to_frame().rename(columns={chosen_genre: \"EveryNoise\"}),\n",
    "    subset_word2vec.to_frame().rename(columns={chosen_genre: \"Word2Vec\"}),\n",
    "    subset_everynoise_ranked.to_frame().rename(columns={chosen_genre: \"EveryNoise_rank\"}),\n",
    "    subset_word2vec_ranked.to_frame().rename(columns={chosen_genre: \"Word2Vec_rank\"})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c79052b-97db-45cf-ac44-45f4aee66d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_visualise = reduce(lambda a, b: pd.merge(a, b, left_index=True, right_index=True), parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f555209-79b1-449b-98ba-23d21635ccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in to_visualise.columns:\n",
    "    if not c.endswith(\"_rank\"):\n",
    "        to_visualise[c+\"_score\"] = MinMaxScaler().fit_transform(to_visualise[c].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b16fda-8151-4eb3-8c21-8cc70de07f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_visualise[\"label\"] = (\n",
    "    to_visualise.index.str.title().map(lambda x: x.replace(\" \", \"<br>\"))  + \"<br><br>\" \n",
    "    + \"ENO: \" + to_visualise.EveryNoise_rank.astype(int).astype(str) + \"<br>\" \n",
    "    + \"W2V: \" + to_visualise.Word2Vec_rank.astype(int).astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eef770-7fab-4538-8786-7a1685de879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_visualise.sort_values(by=\"EveryNoise_rank\", inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b1d93-bde6-4b47-a025-a56c4d44b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding co-occurrence similarities as an additional reference\n",
    "co_occurrence_lookup = pd.read_csv(\n",
    "    CO_OCCURRENCE_TABLE,\n",
    "    index_col=[0, 1]\n",
    ")\n",
    "co_occurrence_lookup.columns = [\"CoOccurrenceValue\"]\n",
    "\n",
    "mask = co_occurrence_lookup.index.get_level_values(0) == chosen_genre.lower()\n",
    "mask |= co_occurrence_lookup.index.get_level_values(1) == chosen_genre.lower()\n",
    "\n",
    "df_co_occurrence = co_occurrence_lookup.loc[mask].reset_index()\n",
    "\n",
    "levels = [i for i in df_co_occurrence if \"level\" in i]\n",
    "df_co_occurrence[\"genre\"] = df_co_occurrence[levels].apply(lambda row: \"\".join([i for i in row if i != chosen_genre.lower()]), axis=1)\n",
    "df_co_occurrence.drop(columns=levels, inplace=True)\n",
    "df_co_occurrence[\"genre\"] = df_co_occurrence[\"genre\"].str.title()\n",
    "df_co_occurrence = df_co_occurrence.loc[df_co_occurrence.genre.isin(to_visualise.index)].copy()\n",
    "df_co_occurrence[\"similarity\"] = MinMaxScaler().fit_transform(df_co_occurrence.CoOccurrenceValue.values.reshape(-1, 1))\n",
    "mapping_genre_co_occurrence = df_co_occurrence.set_index(\"genre\")[\"similarity\"].to_dict()\n",
    "to_visualise[\"Co_Occurrence_score\"] = to_visualise.index.map(mapping_genre_co_occurrence).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019257a1-9e80-48b0-813b-25e653675fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[\n",
    "        {'type': 'polar'}\n",
    "    ]],\n",
    "    column_widths=[1]\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatterpolar(\n",
    "        r=to_visualise.EveryNoise_score,\n",
    "        theta=to_visualise.label,\n",
    "        name=\"Every Noise At Once\",\n",
    "        fill=\"toself\",\n",
    "        opacity=0.4\n",
    "    ), \n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatterpolar(\n",
    "        r=to_visualise.Word2Vec_score,\n",
    "        theta=to_visualise.label,\n",
    "        name=\"Word2Vec\",\n",
    "        fill=\"toself\",\n",
    "        opacity=0.4\n",
    "    ), \n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatterpolar(\n",
    "        r=to_visualise.Co_Occurrence_score,\n",
    "        theta=to_visualise.label,\n",
    "        name=\"Co-Occurrence\",\n",
    "        fill=\"toself\",\n",
    "        opacity=0.4\n",
    "    ), \n",
    "    row=1, col=1\n",
    ")\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    title=f\"Similarities for {chosen_genre.title()}\",\n",
    "    plot_bgcolor='rgb(255,255,255)',\n",
    "    polar=dict(\n",
    "        radialaxis=dict(showticklabels=False, ticks=''),\n",
    "    ),\n",
    "    height=1600,\n",
    "    width=1400,\n",
    "    font=dict(\n",
    "        family=\"avenir, sans serif\",\n",
    "        size=18,\n",
    "        color=\"#334c57\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe3f1ce-18fb-4f3a-84c5-86c6f3895941",
   "metadata": {},
   "source": [
    "**How to read the graph**: For a selected genre (here `Rap`) the graph visualises the similarity metrics obtained under the two approaches.\n",
    "Similarity metrics have been scaled to fall in the range of [0, 1], such that the most similar genre to the selected one has a similarity value of 1.\n",
    "Note that since the two approaches use different formulas to obtain similarities (cosine vs euclidean), the distributions are different.\n",
    "Specifically, Word2Vec generally attributes higher similarities to genre pairs. So the difference between a more similar and a less similar genre are not as pronounced as for Every Noise At Once. The tick labels provide the associated similarity ranking of a given genre, for both approaches. For example, for Every Noise at Once (ENO), Trap is the most similar genre to rap of the considered genres. Therefore, its rank is 1.\n",
    "\n",
    "**Interpretation**: The two approaches agree on the similarity of genres only in a few instances.For example, both consider `Rap` as being failry similar to `Hip Pop`, `Hip Hop` and `Lating Hip Hop`. However, W2V considers `Jazz` to be the 3rd most similar genre to `Rap` out of all the selected genres, which is difficult to subjectively support. Also, ENO correctly identifies the similarities between `Rap` and `Trap`, which is not captured by W2V at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe869ef-227e-4f22-a8b5-b50df1f93610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Github: visualise a static version of the above Plotly figure\n",
    "from IPython.display import Image\n",
    "img_bytes = fig.to_image(format=\"png\")\n",
    "Image(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd96d2b-89f4-4a2f-8e1b-22c1e9330c16",
   "metadata": {},
   "source": [
    "## Comparison of all genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5432beb-c068-4df2-b421-7be37eec9dbd",
   "metadata": {},
   "source": [
    "Based on the concept of comparing the ranking of genres by similarity under both approaches. Let's create a visualisation that summarises similarities across all genres at once. To this end, we will plot a matrix, with each cell corresponding to a pair of genres, i.e. $M_{ij} = (g_i, g_j)$ which takes on as values the difference in similarity ranking, i.e. $\\text{Ranking}_{W2V}(g_i) - \\text{Ranking}_{ENO}(g_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fa4c9-1ca7-4fb6-b63d-4d3abb4c7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://plotly.com/python/annotated-heatmap/\n",
    "fig = px.imshow(\n",
    "    (df_word2vec_ranked - df_everynoise_ranked).abs(),\n",
    "    labels=dict(x=\"Genre\", y=\"Genre\", color=\"Ranking Difference\"),\n",
    "    width=900,\n",
    "    height=900,\n",
    "    zmax=N // 2  # Saturate graph above this value because two methods really disagree on genre similaritya\n",
    ")\n",
    "fig.update_traces(\n",
    "    customdata=np.moveaxis([df_word2vec_ranked, df_everynoise_ranked.astype(int)], 0, -1), \n",
    "    hovertemplate=\"\"\"\n",
    "    <b>Genre pair:</b> %{x} â€“ %{y}<br>\n",
    "    <b>Ranking Difference:</b> %{z}<br>\n",
    "    <b>Ranking Word2Vec vs Everynoise:</b> %{customdata[0]} vs %{customdata[1]}<br>\n",
    "    \"\"\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202626cd-8e2d-43c8-96e1-e784b29a19c5",
   "metadata": {},
   "source": [
    "**How to read the graph**: Every cell's colour represents the difference in W2V and ENO rankings for a specific genre pair, which is defined by the y-axis and x-axis ticks. The more W2V and ENO dissagree on the similarity of a specific genre pair, the brighter the colour.\n",
    "\n",
    "**Interpretation**: Looking at the graph we see that W2V and ENO do not really agree on genre similarities across the board. For `Rap`, we find back the differences from the previous radial plot, specifically pronounced for `Trap` and `Jazz`. In general, we see that `R&B` and `Jazz` are really much treated differently by W2V and ENO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114714f5-55df-47dc-b955-591b2a7dafcc",
   "metadata": {},
   "source": [
    "**Conclusion**: `word2vec` leads to genre similarities that are quite different to those obtained from `everynoise`, which one can treat as capturing genre similarities rather well. Therefore, using a pre-trained `word2vec` model without fine-tuning is not a very good approach to measuring genre similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6abe667-2378-42bd-9171-d7e4019fb46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Github: visualise a static version of the above Plotly figure\n",
    "from IPython.display import Image\n",
    "img_bytes = fig.to_image(format=\"png\")\n",
    "Image(img_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954dc8ec-06e5-4271-a895-086d667408ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
